{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import math\n",
    "from pathlib import Path\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    }
   ],
   "source": [
    "import torchtext.datasets as datasets\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Huggingface datasets and tokenizers\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "import torchmetrics\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbeddings(nn.Module):\n",
    "    def __init__(self, d_model: int, vocab_size: int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, seq: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq = seq\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        pe = torch.zeros(seq, d_model)\n",
    "        position = torch.arange(0, seq, dtype=torch.float).unsqueeze(1) # (seq, 1)\n",
    "\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # (d_model / 2)\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term) # sin(position * (10000 ** (2i / d_model))\n",
    "\n",
    "        pe[:, 1::2] = torch.cos(position * div_term) # cos(position * (10000 ** (2i / d_model))\n",
    "\n",
    "        pe = pe.unsqueeze(0) # (1, seq, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False) # (batch, seq, d_model)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, eps:float=10**-6) -> None:\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.alpha = nn.Parameter(torch.ones(features))\n",
    "        self.bias = nn.Parameter(torch.zeros(features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq, hidden_size)\n",
    "        mean = x.mean(dim = -1, keepdim = True) # (batch, seq, 1)\n",
    "        std = x.std(dim = -1, keepdim = True) # (batch, seq, 1)\n",
    "        return self.alpha * (x - mean) / (std + self.eps) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff) # w1 and b1\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model) # w2 and b2\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch, seq, d_model) --> (batch, seq, d_ff) --> (batch, seq, d_model)\n",
    "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, h: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model # Embedding vector size\n",
    "        self.h = h # Number of heads\n",
    "        assert d_model % h == 0, \"d_model is not divisible by h\"\n",
    "\n",
    "        self.d_k = d_model // h  # split embedding vector for each head\n",
    "        self.w_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.w_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.w_v = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.w_o = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    @staticmethod\n",
    "    def attention(query, key, value, mask, dropout: nn.Dropout):\n",
    "        d_k = query.shape[-1]\n",
    "        # (batch, h, seq, d_k) --> (batch, h, seq, seq)\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            # Low value represents -inf from paper\n",
    "            attention_scores.masked_fill_(mask == 0, -1e9)\n",
    "        attention_scores = attention_scores.softmax(dim=-1) # (batch, h, seq, seq)\n",
    "        if dropout is not None:\n",
    "            attention_scores = dropout(attention_scores)\n",
    "        # (batch, h, seq, seq) --> (batch, h, seq, d_k)\n",
    "        return (attention_scores @ value), attention_scores\n",
    "\n",
    "    def forward(self, q, k, v, mask):\n",
    "        query = self.w_q(q) # (batch, seq, d_model) --> (batch, seq, d_model)\n",
    "        key = self.w_k(k) # (batch, seq, d_model) --> (batch, seq, d_model)\n",
    "        value = self.w_v(v) # (batch, seq, d_model) --> (batch, seq, d_model)\n",
    "\n",
    "        # (batch, seq, d_model) --> (batch, seq, h, d_k) --> (batch, h, seq, d_k)\n",
    "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "\n",
    "        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n",
    "\n",
    "        # Combine all the heads together\n",
    "        # (batch, h, seq, d_k) --> (batch, seq, h, d_k) --> (batch, seq, d_model)\n",
    "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
    "\n",
    "        # (batch, seq, d_model) --> (batch, seq, d_model)\n",
    "        return self.w_o(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "\n",
    "        def __init__(self, features: int, dropout: float) -> None:\n",
    "            super().__init__()\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "            self.norm = LayerNormalization(features)\n",
    "\n",
    "        def forward(self, x, sublayer):\n",
    "            return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(2)])\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n",
    "        x = self.residual_connections[1](x, self.feed_forward_block)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module): # to stack all encoder blocks\n",
    "\n",
    "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization(features)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.cross_attention_block = cross_attention_block # cross-attention for connecting relations between encoder output (input) and target\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(3)])\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n",
    "        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n",
    "        x = self.residual_connections[2](x, self.feed_forward_block)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization(features)\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, vocab_size) -> None:\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x) -> None:\n",
    "        # (batch, seq, d_model) --> (batch, seq, vocab_size)\n",
    "        return self.proj(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbeddings, tgt_embed: InputEmbeddings, src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, projection_layer: ProjectionLayer) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.src_pos = src_pos\n",
    "        self.tgt_pos = tgt_pos\n",
    "        self.projection_layer = projection_layer\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        # (batch, seq, d_model)\n",
    "        src = self.src_embed(src)\n",
    "        src = self.src_pos(src)\n",
    "        return self.encoder(src, src_mask)\n",
    "\n",
    "    def decode(self, encoder_output: torch.Tensor, src_mask: torch.Tensor, tgt: torch.Tensor, tgt_mask: torch.Tensor):\n",
    "        # (batch, seq, d_model)\n",
    "        tgt = self.tgt_embed(tgt)\n",
    "        tgt = self.tgt_pos(tgt)\n",
    "        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
    "\n",
    "    def project(self, x):\n",
    "        # (batch, seq, vocab_size)\n",
    "        return self.projection_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transformer(src_vocab_size: int, tgt_vocab_size: int, src_seq: int, tgt_seq: int, d_model: int=512, N: int=6, h: int=8, dropout: float=0.1, d_ff: int=2048) -> Transformer:\n",
    "    src_embed = InputEmbeddings(d_model, src_vocab_size)\n",
    "    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size)\n",
    "\n",
    "    src_pos = PositionalEncoding(d_model, src_seq, dropout)\n",
    "    tgt_pos = PositionalEncoding(d_model, tgt_seq, dropout)\n",
    "\n",
    "    encoder_blocks = []\n",
    "    for _ in range(N):\n",
    "        encoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "        encoder_block = EncoderBlock(d_model, encoder_self_attention_block, feed_forward_block, dropout)\n",
    "        encoder_blocks.append(encoder_block)\n",
    "\n",
    "    decoder_blocks = []\n",
    "    for _ in range(N):\n",
    "        decoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "        decoder_block = DecoderBlock(d_model, decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n",
    "        decoder_blocks.append(decoder_block)\n",
    "\n",
    "    encoder = Encoder(d_model, nn.ModuleList(encoder_blocks))\n",
    "    decoder = Decoder(d_model, nn.ModuleList(decoder_blocks))\n",
    "\n",
    "    projection_layer = ProjectionLayer(d_model, tgt_vocab_size)\n",
    "\n",
    "    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)\n",
    "\n",
    "    for p in transformer.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "\n",
    "    return transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "\n",
    "    def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq):\n",
    "        super().__init__()\n",
    "        self.seq = seq\n",
    "\n",
    "        self.ds = ds\n",
    "        self.tokenizer_src = tokenizer_src\n",
    "        self.tokenizer_tgt = tokenizer_tgt\n",
    "        self.src_lang = src_lang\n",
    "        self.tgt_lang = tgt_lang\n",
    "\n",
    "        self.sos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[SOS]\")], dtype=torch.int64)\n",
    "        self.eos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[EOS]\")], dtype=torch.int64)\n",
    "        self.pad_token = torch.tensor([tokenizer_tgt.token_to_id(\"[PAD]\")], dtype=torch.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_target_pair = self.ds[idx]\n",
    "        src_text = src_target_pair['translation'][self.src_lang]\n",
    "        tgt_text = src_target_pair['translation'][self.tgt_lang]\n",
    "\n",
    "        enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n",
    "        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n",
    "\n",
    "        enc_num_padding_tokens = self.seq - len(enc_input_tokens) - 2\n",
    "        dec_num_padding_tokens = self.seq - len(dec_input_tokens) - 1\n",
    "\n",
    "        if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n",
    "            raise ValueError(\"Sentence is too long\")\n",
    "\n",
    "        encoder_input = torch.cat(\n",
    "            [\n",
    "                self.sos_token,\n",
    "                torch.tensor(enc_input_tokens, dtype=torch.int64),\n",
    "                self.eos_token,\n",
    "                torch.tensor([self.pad_token] * enc_num_padding_tokens, dtype=torch.int64),\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        decoder_input = torch.cat(\n",
    "            [\n",
    "                self.sos_token,\n",
    "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
    "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        # Add only </s> token\n",
    "        label = torch.cat(\n",
    "            [\n",
    "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
    "                self.eos_token,\n",
    "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        # Double check the size of the tensors to make sure they are all seq long\n",
    "        assert encoder_input.size(0) == self.seq\n",
    "        assert decoder_input.size(0) == self.seq\n",
    "        assert label.size(0) == self.seq\n",
    "\n",
    "        return {\n",
    "            \"encoder_input\": encoder_input,  # (seq)\n",
    "            \"decoder_input\": decoder_input,  # (seq)\n",
    "            \"encoder_mask\": (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(), # (1, 1, seq)\n",
    "            \"decoder_mask\": (decoder_input != self.pad_token).unsqueeze(0).int() & causal_mask(decoder_input.size(0)), # (1, seq) & (1, seq, seq),\n",
    "            \"label\": label,  # (seq)\n",
    "            \"src_text\": src_text,\n",
    "            \"tgt_text\": tgt_text,\n",
    "        }\n",
    "\n",
    "def causal_mask(size):\n",
    "    mask = torch.triu(torch.ones((1, size, size)), diagonal=1).type(torch.int)\n",
    "    return mask == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config():\n",
    "    return {\n",
    "        \"batch_size\": 8,\n",
    "        \"num_epochs\": 20,\n",
    "        \"lr\": 10**-4,\n",
    "        \"seq\": 350,\n",
    "        \"d_model\": 512,\n",
    "        \"datasource\": 'opus_books',\n",
    "        \"lang_src\": \"en\",\n",
    "        \"lang_tgt\": \"it\",\n",
    "        \"model_folder\": \"weights\",\n",
    "        \"model_basename\": \"tmodel_\",\n",
    "        \"preload\": \"latest\",\n",
    "        \"tokenizer_file\": \"tokenizer_{0}.json\",\n",
    "        \"experiment_name\": \"runs/tmodel\"\n",
    "    }\n",
    "\n",
    "def get_weights_file_path(config, epoch: str):\n",
    "    model_folder = f\"{config['datasource']}_{config['model_folder']}\"\n",
    "    model_filename = f\"{config['model_basename']}{epoch}.pt\"\n",
    "    return str(Path('.') / model_folder / model_filename)\n",
    "\n",
    "# Find the latest weights file in the weights folder\n",
    "def latest_weights_file_path(config):\n",
    "    model_folder = f\"{config['datasource']}_{config['model_folder']}\"\n",
    "    model_filename = f\"{config['model_basename']}*\"\n",
    "    weights_files = list(Path(model_folder).glob(model_filename))\n",
    "    if len(weights_files) == 0:\n",
    "        return None\n",
    "    weights_files.sort()\n",
    "    return str(weights_files[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_sentences(ds, lang):\n",
    "    for item in ds:\n",
    "        yield item['translation'][lang]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_build_tokenizer(config, ds, lang):\n",
    "    tokenizer_path = Path(config['tokenizer_file'].format(lang))\n",
    "    if not Path.exists(tokenizer_path):\n",
    "        # Most code taken from: https://huggingface.co/docs/tokenizers/quicktour\n",
    "        tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "        tokenizer.pre_tokenizer = Whitespace()\n",
    "        trainer = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=2)\n",
    "        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)\n",
    "        tokenizer.save(str(tokenizer_path))\n",
    "    else:\n",
    "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ds(config):\n",
    "    ds_raw = load_dataset(f\"{config['datasource']}\", f\"{config['lang_src']}-{config['lang_tgt']}\", split='train')\n",
    "\n",
    "    tokenizer_src = get_or_build_tokenizer(config, ds_raw, config['lang_src'])\n",
    "    tokenizer_tgt = get_or_build_tokenizer(config, ds_raw, config['lang_tgt'])\n",
    "\n",
    "    # Keep 90% for training, 10% for validation\n",
    "    train_ds_size = int(0.9 * len(ds_raw))\n",
    "    val_ds_size = len(ds_raw) - train_ds_size\n",
    "    train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size])\n",
    "\n",
    "    train_ds = TranslationDataset(train_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq'])\n",
    "    val_ds = TranslationDataset(val_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq'])\n",
    "\n",
    "    # Find the maximum length of each sentence in the source and target sentence\n",
    "    max_len_src = 0\n",
    "    max_len_tgt = 0\n",
    "\n",
    "    for item in ds_raw:\n",
    "        src_ids = tokenizer_src.encode(item['translation'][config['lang_src']]).ids\n",
    "        tgt_ids = tokenizer_tgt.encode(item['translation'][config['lang_tgt']]).ids\n",
    "        max_len_src = max(max_len_src, len(src_ids))\n",
    "        max_len_tgt = max(max_len_tgt, len(tgt_ids))\n",
    "\n",
    "    print(f'Max length of source sentence: {max_len_src}')\n",
    "    print(f'Max length of target sentence: {max_len_tgt}')\n",
    "\n",
    "    train_dataloader = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True)\n",
    "    val_dataloader = DataLoader(val_ds, batch_size=1, shuffle=True)\n",
    "\n",
    "    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(config, vocab_src_len, vocab_tgt_len):\n",
    "    model = build_transformer(vocab_src_len, vocab_tgt_len, config[\"seq\"], config['seq'], d_model=config['d_model'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device):\n",
    "    sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n",
    "    eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n",
    "\n",
    "    encoder_output = model.encode(source, source_mask)\n",
    "    decoder_input = torch.empty(1, 1).fill_(sos_idx).type_as(source).to(device)\n",
    "    while True:\n",
    "        if decoder_input.size(1) == max_len:\n",
    "            break\n",
    "\n",
    "        # build mask for target\n",
    "        decoder_mask = causal_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n",
    "\n",
    "        # calculate output\n",
    "        out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n",
    "\n",
    "        # get next token\n",
    "        prob = model.project(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        decoder_input = torch.cat(\n",
    "            [decoder_input, torch.empty(1, 1).type_as(source).fill_(next_word.item()).to(device)], dim=1\n",
    "        )\n",
    "\n",
    "        if next_word == eos_idx:\n",
    "            break\n",
    "\n",
    "    return decoder_input.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_validation(model, validation_ds, tokenizer_src, tokenizer_tgt, max_len, device, print_msg, global_step, writer, num_examples=2):\n",
    "    model.eval()\n",
    "    count = 0\n",
    "\n",
    "    source_texts = []\n",
    "    expected = []\n",
    "    predicted = []\n",
    "\n",
    "    try:\n",
    "        with os.popen('stty size', 'r') as console:\n",
    "            _, console_width = console.read().split()\n",
    "            console_width = int(console_width)\n",
    "    except:\n",
    "        # If we can't get the console width, use 80 as default\n",
    "        console_width = 80\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in validation_ds:\n",
    "            count += 1\n",
    "            encoder_input = batch[\"encoder_input\"].to(device) # (b, seq)\n",
    "            encoder_mask = batch[\"encoder_mask\"].to(device) # (b, 1, 1, seq)\n",
    "\n",
    "            # check that the batch size is 1\n",
    "            assert encoder_input.size(\n",
    "                0) == 1, \"Batch size must be 1 for validation\"\n",
    "\n",
    "            model_out = greedy_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n",
    "\n",
    "            source_text = batch[\"src_text\"][0]\n",
    "            target_text = batch[\"tgt_text\"][0]\n",
    "            model_out_text = tokenizer_tgt.decode(model_out.detach().cpu().numpy())\n",
    "\n",
    "            source_texts.append(source_text)\n",
    "            expected.append(target_text)\n",
    "            predicted.append(model_out_text)\n",
    "            \n",
    "            print_msg('-'*console_width)\n",
    "            print_msg(f\"{f'SOURCE: ':>12}{source_text}\")\n",
    "            print_msg(f\"{f'TARGET: ':>12}{target_text}\")\n",
    "            print_msg(f\"{f'PREDICTED: ':>12}{model_out_text}\")\n",
    "\n",
    "            if count == num_examples:\n",
    "                print_msg('-'*console_width)\n",
    "                break\n",
    "    \n",
    "    if writer:\n",
    "        # Evaluate the character error rate\n",
    "        # Compute the char error rate \n",
    "        metric = torchmetrics.CharErrorRate()\n",
    "        cer = metric(predicted, expected)\n",
    "        writer.add_scalar('validation cer', cer, global_step)\n",
    "        writer.flush()\n",
    "\n",
    "        # Compute the word error rate\n",
    "        metric = torchmetrics.WordErrorRate()\n",
    "        wer = metric(predicted, expected)\n",
    "        writer.add_scalar('validation wer', wer, global_step)\n",
    "        writer.flush()\n",
    "\n",
    "        # Compute the BLEU metric\n",
    "        metric = torchmetrics.BLEUScore()\n",
    "        bleu = metric(predicted, expected)\n",
    "        writer.add_scalar('validation BLEU', bleu, global_step)\n",
    "        writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config):\n",
    "    # Define the device\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.has_mps or torch.backends.mps.is_available() else \"cpu\"\n",
    "    print(\"Using device:\", device)\n",
    "    if (device == 'cuda'):\n",
    "        print(f\"Device name: {torch.cuda.get_device_name(device.index)}\")\n",
    "        print(f\"Device memory: {torch.cuda.get_device_properties(device.index).total_memory / 1024 ** 3} GB\")\n",
    "    elif (device == 'mps'):\n",
    "        print(f\"Device name: <mps>\")\n",
    "    else:\n",
    "        print(\"NOTE: If you have a GPU, consider using it for training.\")\n",
    "        print(\"      On a Windows machine with NVidia GPU, check this video: https://www.youtube.com/watch?v=GMSjDTU8Zlc\")\n",
    "        print(\"      On a Mac machine, run: pip3 install --pre torch torchvision torchaudio torchtext --index-url https://download.pytorch.org/whl/nightly/cpu\")\n",
    "    device = torch.device(device)\n",
    "\n",
    "    # Make sure the weights folder exists\n",
    "    Path(f\"{config['datasource']}_{config['model_folder']}\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n",
    "    model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n",
    "    # Tensorboard\n",
    "    writer = SummaryWriter(config['experiment_name'])\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps=1e-9)\n",
    "\n",
    "    # If the user specified a model to preload before training, load it\n",
    "    initial_epoch = 0\n",
    "    global_step = 0\n",
    "    preload = config['preload']\n",
    "    model_filename = latest_weights_file_path(config) if preload == 'latest' else get_weights_file_path(config, preload) if preload else None\n",
    "    if model_filename:\n",
    "        print(f'Preloading model {model_filename}')\n",
    "        state = torch.load(model_filename)\n",
    "        model.load_state_dict(state['model_state_dict'])\n",
    "        initial_epoch = state['epoch'] + 1\n",
    "        optimizer.load_state_dict(state['optimizer_state_dict'])\n",
    "        global_step = state['global_step']\n",
    "    else:\n",
    "        print('No model to preload, starting from scratch')\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_src.token_to_id('[PAD]'), label_smoothing=0.1).to(device)\n",
    "\n",
    "    for epoch in range(initial_epoch, config['num_epochs']):\n",
    "        torch.cuda.empty_cache()\n",
    "        model.train()\n",
    "        batch_iterator = tqdm(train_dataloader, desc=f\"Processing Epoch {epoch:02d}\")\n",
    "        for batch in batch_iterator:\n",
    "\n",
    "            encoder_input = batch['encoder_input'].to(device) # (b, seq)\n",
    "            decoder_input = batch['decoder_input'].to(device) # (B, seq)\n",
    "            encoder_mask = batch['encoder_mask'].to(device) # (B, 1, 1, seq)\n",
    "            decoder_mask = batch['decoder_mask'].to(device) # (B, 1, seq, seq)\n",
    "\n",
    "            encoder_output = model.encode(encoder_input, encoder_mask) # (B, seq, d_model)\n",
    "            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask) # (B, seq, d_model)\n",
    "            proj_output = model.project(decoder_output) # (B, seq, vocab_size)\n",
    "\n",
    "            label = batch['label'].to(device) # (B, seq)\n",
    "\n",
    "            loss = loss_fn(proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))\n",
    "            batch_iterator.set_postfix({\"loss\": f\"{loss.item():6.3f}\"})\n",
    "\n",
    "            writer.add_scalar('train loss', loss.item(), global_step)\n",
    "            writer.flush()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            global_step += 1\n",
    "\n",
    "        run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq'], device, lambda msg: batch_iterator.write(msg), global_step, writer)\n",
    "\n",
    "        model_filename = get_weights_file_path(config, f\"{epoch:02d}\")\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'global_step': global_step\n",
    "        }, model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Device name: NVIDIA GeForce RTX 3060 Laptop GPU\n",
      "Device memory: 5.99951171875 GB\n",
      "Max length of source sentence: 309\n",
      "Max length of target sentence: 274\n",
      "No model to preload, starting from scratch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 00: 100%|██████████| 3638/3638 [1:09:58<00:00,  1.15s/it, loss=5.109]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: 'Go with this note to the Countess Vronskaya's country house; do you know it?\n",
      "    TARGET: — Va’ con questo stesso biglietto, in campagna, dalla contessa Vronskaja, sai?\n",
      " PREDICTED: — Non è vero , ma non è vero , ma non è vero ?\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: It was long since the farm work had seemed so important to him as it did that evening.\n",
      "    TARGET: Da tempo gli affari dell’azienda non gli apparivano così importanti come quel giorno.\n",
      " PREDICTED: a un ’ altra , ma non era più stato , ma non si .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 01: 100%|██████████| 3638/3638 [1:04:31<00:00,  1.06s/it, loss=5.993]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: When they were still a quarter of a verst from the house, Levin saw Grisha and Tanya running toward him.\n",
      "    TARGET: Circa un quarto di versta prima di giungere a casa, Levin scorse Tanja e Griša che gli correvano incontro.\n",
      " PREDICTED: Quando si , , , , Levin , Levin si fermò e Levin si fermò , e Levin si fermò .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Through two luminous windows saw\n",
      "    TARGET: attraverso a due finestre luminose,\n",
      " PREDICTED: .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 02: 100%|██████████| 3638/3638 [1:14:12<00:00,  1.22s/it, loss=4.736]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: And of all princes, it is impossible for the new prince to avoid the imputation of cruelty, owing to new states being full of dangers.\n",
      "    TARGET: Et intra tutti e' principi, al principe nuovo è impossibile fuggire el nome di crudele, per essere li stati nuovi pieni di pericoli.\n",
      " PREDICTED: E che , per ' tempi è stato di non essere , et , et , et , et .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: You can never rouse Harris.\n",
      "    TARGET: Noi non possiamo mai scuotere Harris.\n",
      " PREDICTED: Non vi avete detto nulla di Harris .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 03: 100%|██████████| 3638/3638 [1:12:26<00:00,  1.19s/it, loss=5.624]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: They prayed: 'That they may live in chastity for the good of the fruits of the womb, and find joy in their sons and daughters.'\n",
      "    TARGET: Si pregava “perché fosse loro donata la purezza, e il frutto delle viscere per il loro bene, perché si rallegrassero della vista dei figli e delle figlie”.\n",
      " PREDICTED: : “ è sempre sempre più di , di , e di , e di e di .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: She was dressed in pure white; an amber-coloured scarf was passed over her shoulder and across her breast, tied at the side, and descending in long, fringed ends below her knee.\n",
      "    TARGET: Aveva un vestito bianco, una sciarpa color ambra gettata sulle spalle.\n",
      " PREDICTED: Ella era un ' espressione di capelli e , e la mano era , e la mano si alzò e si mise a guardare il viso , e si , , si , si , il capo .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 04: 100%|██████████| 3638/3638 [1:04:49<00:00,  1.07s/it, loss=4.891]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: The Countess Lydia Ivanovna knew very well that it was one of his greatest pleasures, though he would never confess it.\n",
      "    TARGET: La contessa Lidija Ivanovna sapeva bene che questa era una delle sue più grandi gioie, anche se egli non l’avrebbe mai confessato.\n",
      " PREDICTED: La contessa Lidija Ivanovna aveva una volta che era una più grande , ma non poteva mai mai .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: He had been to Spain, where he arranged serenades and became intimate with a Spanish woman who played the mandoline.\n",
      "    TARGET: Era stato in Spagna, e là aveva fatto serenate e stretto amicizia con una spagnola che sonava il mandolino.\n",
      " PREDICTED: Era stato per , dove si con uno di , e di con un di .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 05: 100%|██████████| 3638/3638 [1:14:14<00:00,  1.22s/it, loss=4.308]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: It trotted quietly on until its would-be assassin was within a yard of it, and then it turned round and sat down in the middle of the road, and looked at Montmorency with a gentle, inquiring expression, that said:\n",
      "    TARGET: Trotterellò tranquillamente finchè l’eventuale assassino non si trovò a un metro da lui; e poi si voltò e si sedette in mezzo alla strada, guardando Montmorency con una soave, interrogativa espressione, che diceva:\n",
      " PREDICTED: Era un po ’ di sollievo , se fosse stato più difficile di più , e poi si alzò in piedi , e si fermò a piedi , e si fermò con un ’ espressione di gioia , che diceva :\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: She did not doubt that she had acted rightly, yet for a long time she lay in bed unable to sleep.\n",
      "    TARGET: Non aveva nessun dubbio di non essersi regolata così come conveniva. Ma a letto, per molto tempo, non poté prendere sonno.\n",
      " PREDICTED: Non era vero che non aveva pensato , ma , non aveva visto neppure una volta , non poteva dormire .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 06: 100%|██████████| 3638/3638 [1:11:52<00:00,  1.19s/it, loss=4.403]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: And this being an ancient custom, it cannot be called a new principality, because there are none of those difficulties in it that are met with in new ones; for although the prince is new, the constitution of the state is old, and it is framed so as to receive him as if he were its hereditary lord.\n",
      "    TARGET: Et essendo questo ordine antiquato, non si può chiamare principato nuovo, perché in quello non sono alcune di quelle difficultà che sono ne' nuovi; perché, se bene el principe è nuovo, li ordini di quello stato sono vecchi et ordinati a riceverlo come se fussi loro signore ereditario.\n",
      " PREDICTED: E questa è una grande , non può essere , perché non sono mai stati che quelli che sono stati in Italia , e che con le cose sono , come si , e come se non si , e che , come se non si .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: His softened voice announced that he was subdued; so I, in my turn, became calm.\n",
      "    TARGET: La voce addolcita mi diceva che egli si era calmato e io divenni più tranquilla.\n",
      " PREDICTED: Le sue parole mi che mi , e mi accorsi che era così forte .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 07: 100%|██████████| 3638/3638 [1:04:31<00:00,  1.06s/it, loss=3.874]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: In that short time the centre of the cloud had already so moved over the sun that it was as dark as during an eclipse.\n",
      "    TARGET: In quel breve spazio di tempo la nuvola s’era già tanto avanzata col suo centro sul sole, che s’era fatto buio come in una eclissi.\n",
      " PREDICTED: In quel momento il più piccolo della luce , si era nel sole che il sole era come un matrimonio .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: But when I had done this, I was unable to stir it up again, or to get under it, much less to move it forward towards the water; so I was forced to give it over; and yet, though I gave over the hopes of the boat, my desire to venture over for the main increased, rather than decreased, as the means for it seemed impossible.\n",
      "    TARGET: Ma raggiunta questa meta, mi trovai nuovamente inabile a moverla, a mettermici sotto, tanto più poi a spingerla in acqua, onde finalmente fui costretto ad abbandonar la mia impresa. Pure, anche perdute tutte le speranze ch’io avea riposte nella scialuppa, cresceva in me, anzichè diminuire, il desiderio d’avventurarmi verso la terra comparsami innanzi, e crescea con tanta maggior forza quanto più impossibile ne apparivano i mezzi.\n",
      " PREDICTED: Ma quando io mi avea fatto , non mi riuscì a , nè di più in un ’ altra volta mi riuscì per ; ma io fui pure di più presto , perchè non mi riuscì a ; ma benchè il tempo mi la mia navicella , se non avessi potuto più forte di quella scialuppa , non mi riuscì più di quanto mi fosse possibile .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 08: 100%|██████████| 3638/3638 [1:04:40<00:00,  1.07s/it, loss=3.666]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: He had been sitting, without knowing it, on the very verge of a small gully, the long grass hiding it from view; and in leaning a little back he had shot over, pie and all.\n",
      "    TARGET: Egli s’era seduto, senza accorgersene, sul ciglio d’un fosso che la lunga erba nascondeva alla vista, e, nel tirarsi indietro, v’era precipitato col pasticcio e tutto.\n",
      " PREDICTED: Aveva appena appena seduto , senza capire , in alto , in un piccolo , un po ’ di , di sotto gli alberi , e di un pezzo , lo fece in fretta , e tutto il pasticcio .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: It is unpleasant for me to enter this house.\n",
      "    TARGET: “Viene lei da me — pensò Vronskij — e sarebbe meglio.\n",
      " PREDICTED: È vero che mi è accaduto questo a casa .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 09: 100%|██████████| 3638/3638 [1:04:45<00:00,  1.07s/it, loss=3.113]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: This was hardly what I intended.\n",
      "    TARGET: Veramente io non la intendevo così.\n",
      " PREDICTED: Non avevo nulla da fare .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: He loves (as he _can_ love, and that is not as you love) a beautiful young lady called Rosamond.\n",
      "    TARGET: \"Egli è innamorato (come lui può esserlo) di una bella ragazza, per nome Rosmunda.\n",
      " PREDICTED: Egli ha detto che fare amore , e che non è amore , come è giovane , ragazza di ragazza .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 10: 100%|██████████| 3638/3638 [1:14:27<00:00,  1.23s/it, loss=3.349]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: \"Go to the devil!\" was his brother-in-law's recommendation.\n",
      "    TARGET: — Andate al diavolo, — gli disse il cognato.\n",
      " PREDICTED: — il diavolo ! — esclamò il cognato .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: \"What are you going to do?\" asked George's father.\n",
      "    TARGET: — E che decisione prendi? — chiese il padre di Giorgio.\n",
      " PREDICTED: — Che volete fare ? — domandò il padre di Giorgio .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 11: 100%|██████████| 3638/3638 [1:14:27<00:00,  1.23s/it, loss=3.293]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: I never thought it would be so interesting.\n",
      "    TARGET: Non avrei mai pensato che la cosa fosse così interessante.\n",
      " PREDICTED: Non avevo mai pensato a questo .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: 'I wrote both to you and to Sergius Ivanich that I do not know you and do not wish to know you.\n",
      "    TARGET: — Io ho scritto a voi e a Sergej Ivanyc che non vi conosco e non voglio conoscervi.\n",
      " PREDICTED: — Io ti ho scritto e a Sergej Ivanovic , a questo non so che io e non lo voglio .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 12: 100%|██████████| 3638/3638 [1:12:11<00:00,  1.19s/it, loss=3.357]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: She had not expected such cruelty from her, and was angry with her.\n",
      "    TARGET: Non si aspettava tanta crudeltà da lei e ne provò sdegno.\n",
      " PREDICTED: Ella non aspettava in lei un essere simile , e si era arrabbiato con lei .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: The doctor...'\n",
      "    TARGET: Il dottore... e poi...\n",
      " PREDICTED: Il dottore ...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 13: 100%|██████████| 3638/3638 [1:14:19<00:00,  1.23s/it, loss=3.487]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: When the Countess Nordston took the liberty of hinting that she had hoped for something better, Kitty got so heated and proved so convincingly that no one on earth could be better than Levin, that the Countess had to admit it, and thereafter never encountered Levin in Kitty's presence without a smile of delight.\n",
      "    TARGET: Quando la contessa Nordston si permise di accennare al fatto che avrebbe desiderato per lei qualcosa di meglio, Kitty si accalorò tanto e dimostrò con tanta convinzione che non poteva esservi al mondo alcuno migliore di Levin, che la contessa Nordston dovette riconoscerlo e da allora in poi, in presenza di Kitty, accolse Levin con un sorriso di ammirazione.\n",
      " PREDICTED: Quando la contessa Nordston aveva preso la libertà che aveva cercato di trovare qualcosa , Kitty s ’ era preparato a trovare una così e che non si poteva trovare sulla terra , Levin che la contessa Nordston aveva conosciuto e che Levin non aveva mai avuto un sorriso di gioia , e Levin si abbandonò all ’ improvviso , senza un sorriso di gioia .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: They have no conception of what happiness is, and they do not know that without love there is no happiness or unhappiness for us, for there would be no life,' he thought.\n",
      "    TARGET: Non hanno neppure un’idea di che cosa sia la felicità, non sanno che senza questo amore per noi non c’è felicità, né infelicità, non c’è vita” pensava.\n",
      " PREDICTED: Non hanno l ’ unica cosa che sia la felicità , e che non si può , che non ci sia la felicità di noi , così almeno sarebbe stato per noi — pensava Levin .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 14: 100%|██████████| 3638/3638 [1:12:19<00:00,  1.19s/it, loss=3.097]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: And now, lest my good resolutions should continue, my companion, who had enticed me away, comes to me; “Well, Bob,” says he, clapping me upon the shoulder, “how do you do after it?\n",
      "    TARGET: Allora il mio compagno per paura che continuassero le mie buone risoluzioni, perchè era stato egli che m’avea sedotto a fuggire di casa, mi si accosto battendomi amichevolmente con una mano la spalla e dicendomi: — «Ebbene, come vi sentite adesso, bell’uomo?\n",
      " PREDICTED: E ora , se il mio consiglio fosse andato a male , m ’ avesse voluto dire , , anch ’ io : , ! — « , padrone ! me stare a dire !»\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: 'Why, is it a philanthropic undertaking?'\n",
      "    TARGET: — Be’, qualcosa di filantropico?\n",
      " PREDICTED: — Ma perché è un ’ impresa tale ?\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 15: 100%|██████████| 3638/3638 [1:12:02<00:00,  1.19s/it, loss=3.672]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Some years older than I, she knew more of the world, and could tell me many things I liked to hear: with her my curiosity found gratification: to my faults also she gave ample indulgence, never imposing curb or rein on anything I said.\n",
      "    TARGET: Aveva qualche anno più di me, e, conoscendo il mondo, poteva narrarmi cose che mi dilettavano. Anna era indulgente per i miei difetti e non metteva mai un freno alle mie parole.\n",
      " PREDICTED: Qualche anno fa più che mai il mondo di me conosceva o molte cose , che potei il mio dovere . La mia lettera , con la mia lettera , il mio pensiero non era più nulla , né pronta a nessuna forma .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: \"You know--and perhaps think well of.\"\n",
      "    TARGET: — Voi pensate forse....\n",
      " PREDICTED: — Sapete , e credete di esser bene .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 16: 100%|██████████| 3638/3638 [1:12:31<00:00,  1.20s/it, loss=2.408]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: The forest transaction was completed, he had the money in his pocket, the shooting had been fine, Oblonsky was in the best of spirits, and therefore all the more anxious to dispel Levin's ill-humour.\n",
      "    TARGET: L’affare del bosco era concluso, il denaro era in tasca, la caccia era stata magnifica, e Stepan Arkad’ic si trovava nella più amena disposizione d’animo; voleva perciò in particolar modo disperdere il cattivo umore che era piombato su Levin.\n",
      " PREDICTED: Il bosco era passato , fatto il denaro in tasca , il denaro era bello , Stepan Arkad ’ ic era stato ottimo umore e , perciò , tutti , si faceva caldo per Levin .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: CHAPTER XX\n",
      "    TARGET: XX.\n",
      " PREDICTED: XX\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 17: 100%|██████████| 3638/3638 [1:12:39<00:00,  1.20s/it, loss=2.459]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Only a few drops...\n",
      "    TARGET: Appena poche gocce.\n",
      " PREDICTED: Solo alcuni pezzi ...\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: For the rest, whether trite or novel, it is short.\n",
      "    TARGET: \"Del resto è corta.\n",
      " PREDICTED: Per lo stesso , se la signorina o il tempo è breve e breve .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 18: 100%|██████████| 3638/3638 [1:07:59<00:00,  1.12s/it, loss=1.896]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: The old man complained that his affairs were in a bad way.\n",
      "    TARGET: Il vecchio si lamentava che gli affari andavano male.\n",
      " PREDICTED: Il vecchio che gli altri si alzarono .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: 'You insist too much on your devotion, for me to value it greatly,' she replied in the same playful tone, while she involuntarily listened to the sound of Vronsky's footsteps following them.\n",
      "    TARGET: — Tu insisti troppo su questa tua tenerezza, perché io possa apprezzarla — disse lei con lo stesso tono scherzoso, prestando involontariamente orecchio al suono dei passi di Vronskij che camminava dietro di loro.\n",
      " PREDICTED: — Voi dovete volerle bene , per me vi sta bene — ella disse con la stessa maniera sottile nel tono sottile di riso , mentre lei , con un tono tale che guardava Vronskij nel suo silenzio .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 19: 100%|██████████| 3638/3638 [1:16:48<00:00,  1.27s/it, loss=2.320]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: The feeling was not like an electric shock, but it was quite as sharp, as strange, as startling: it acted on my senses as if their utmost activity hitherto had been but torpor, from which they were now summoned and forced to wake.\n",
      "    TARGET: Quella sensazione non somigliava a un urto elettrico, ma era altrettanto acuto, quanto strano e violento. Pareva che fino a quel momento la mia maggior attività fosse stata soltanto torpore, dal quale mi s'imponeva d'uscire.\n",
      " PREDICTED: Il sentimento non era come un , ma nel mio strano , così strano , come si mostrò i miei segni . Dopo aver stabilito , a quanto i miei desideri , si erano messi a vedere prima i miei pensieri .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: 'You don't care much for oysters?' said Oblonsky, emptying his champagne glass – 'or perhaps you're thinking of something else.\n",
      "    TARGET: — Ma non ti piacciono le ostriche? — disse Stepan Arkad’ic vuotando la coppa — o forse sei preoccupato?\n",
      " PREDICTED: — Ma voi non fate nulla per le ostriche ? — disse Stepan Arkad ’ ic , facendo uno scrivere o a tutti gli altri . — Ma forse , a che forse hai ragione ?\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    config = get_config()\n",
    "    train_model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
